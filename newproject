import pandas as pd
import numpy as np
import requests
import joblib
import pycelo
import nltk
nltk.download('punkt')
from transformers import pipeline, AutoTokenizer, TFAutoModelForSequenceClassification
from nltk.tokenize import word_tokenize # Use word_tokenize
from sklearn.linear_model import LogisticRegression # Use LogisticRegression instead of MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer # Use TfidfVectorizer instead of CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from tika import parser # Use Tika instead of PyPDF2
from bs4 import BeautifulSoup

# Carregar o modelo treinado a partir do arquivo
model = joblib.load('modelo.joblib')

# Carregue o modelo pré-treinado BERT e o tokenizador correspondente
model_name = 'neuralmind/bert-base-portuguese-cased'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)

# Defina uma função para processar o texto de entrada e fazer previsões com o modelo
def predict(text):
    inputs = tokenizer(text, padding=True, truncation=True, return_tensors="tf")
    outputs = model(inputs)[0]
    predicted_class = np.argmax(outputs.numpy(), axis=1)[0]
    return predicted_class

# Exemplo de como usar a função de previsão em um novo texto
new_text = "Este é um novo texto que será classificado pelo modelo."
predicted_class = predict(new_text)
print("A classe prevista para o novo texto é:", predicted_class)

# Abrir o arquivo PDF usando Tika
raw = parser.from_file('C:/Users/uryuu/Downloads/DSM-5-TR.pdf')
texto_livro = raw['content']

# Tokenize o texto do livro
tokenizer = word_tokenize
livro_tokens = word_tokenize(texto_livro)

# Crie um dataframe para os dados do livro usando frases em vez de tokens
dados = pd.DataFrame({'texto': livro_tokens.sentences(), 'classe': ['livro']*len(livro_tokens.sentences())})

# Você pode usar qualquer fonte de texto que você quiser, mas aqui eu vou usar alguns textos de exemplo do nltk
from nltk.corpus import gutenberg, brown, reuters

# Adicione mais textos de exemplo de diferentes gêneros e categorias
url_list = [
    "http://pepsic.bvsalud.org/scielo.php?script=sci_arttext&pid=S1415-71282006000200011",
    "https://www.maxwell.vrac.puc-rio.br/15549/15549_6.PDF",
    "https://www.scielo.br/j/fractal/a/KrXHs3m8gWT7JLFWmX6PkDH/",
    "https://www.paho.org/pt/topicos/transtornos-mentais",
    "https://www.scielo.br/j/fractal/a/TVnDqTFkCrLHBHt8Z3BqNvm/"
]

for url in url_list:
    # Fazer uma requisição HTTP ao link do artigo
    response = requests.get(url)

    # Verificar se a requisição foi bem sucedida
    if response.status_code == 200:
        # Fazer o parsing do conteúdo HTML
        soup = BeautifulSoup(response.content, "html.parser")
        # Extrair o texto do artigo
        texto = soup.find("div", {"id": "articleText"}).get_text()
        # Adicionar o texto do artigo ao dataframe dos dados do livro
        dados = dados.append({"texto": texto, "classe": "nao_livro"}, ignore_index=True)
    else:
        # Imprimir uma mensagem de erro
        print("Erro ao acessar o link:", response.status_code)
# Crie o modelo de pipeline e treine com os dados usando BERT
modelo = Pipeline([
    ('vect',TfidfVectorizer()), # Use TfidfVectorizer instead of CountVectorizer
    ('clf', LogisticRegression()), # Use LogisticRegression instead of MultinomialNB
    ])

#Divida os dados em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(dados['texto'], dados['classe'], test_size=0.2, random_state=42)

# Crie um pipeline com TfidfVectorizer e LogisticRegression
text_clf = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('clf', LogisticRegression()),
])

# Treine o modelo com os dados rotulados
text_clf.fit(X_train, y_train)

# Faça previsões em novos documentos
predicted = text_clf.predict(X_test)

#Treine o modelo com os dados de treinamento usando BERT
X_train = modelo['vect'].fit_transform(X_train) # Use fit_transform for training data
modelo.fit(X_train, y_train)

#Faça previsões no conjunto de teste e calcule a acurácia
X_test = modelo['vect'].transform(X_test) # Use transform for test data
y_pred = modelo.predict(X_test) # Use pipeline.predict for test data
accuracy = accuracy_score(y_test, y_pred)
print("Acurácia do modelo:", accuracy)

#Calcule outras métricas além da acurácia
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

#Fazer requisições HTTP aos links adicionais
links_adicionais = ["http://pepsic.bvsalud.org/scielo.php?script=sci_arttext&pid=S1415-71282006000200011",
"https://www.maxwell.vrac.puc-rio.br/15549/15549_6.PDF",
"https://www.scielo.br/j/fractal/a/KrXHs3m8gWT7JLFWmX6PkDH/",
"https://www.paho.org/pt/topicos/transtornos-mentais",
"https://www.scielo.br/j/fractal/a/TVnDqTFkCrLHBHt8Z3BqNvm/"]

for link in links_adicionais:
    response = requests.get(link)

    if response.status_code == 200:
        soup = BeautifulSoup(response.content, "html.parser")
        texto = soup.find("div", {"id": "articleText"}).get_text()
        dados = dados.append({"texto": texto, "classe": "livro"}, ignore_index=True)

    else:
        print("Erro ao acessar o link:", response.status_code)

#Repetir os passos de vetorização, treinamento e teste com os novos dados adicionados
X_train, X_test, y_train, y_test = train_test_split(dados['texto'], dados['classe'], test_size=0.2, random_state=42)
modelo.fit(X_train, y_train)
y_pred = modelo.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Acurácia do modelo com dados adicionais:", accuracy)

# Solicitar ao usuário para inserir um texto
new_text = input("Descreva o caso: ")

# Executar a busca no Scielo e gerar sugestões com base nos resultados da busca
search_suggestions = search_scielo(new_text)

def predict(text):
    # Realizar busca na Scielo
    search_results = search_scielo(text)
    
def search_scielo(search_query):
    scielo = PyCelo()
    scielo.search(search_query)

# Usar a função de previsão para fazer uma previsão de classe para o novo texto
predicted_class = predict(search_query)

# Gerar uma sugestão com base na classe prevista
if predicted_class == 0:
    suggestion = "Sugestão para a classe 0"
elif predicted_class == 1:
    suggestion = "Sugestão para a classe 1"
else:
    suggestion = "Sugestão para a classe 2"

# Combinar a sugestão gerada pela função predict com as sugestões geradas pela função search_scielo
combined_suggestions = suggestion + "\n" + "\n".join(search_suggestions)

# Defina o limite máximo de caracteres para as sugestões
max_chars = 1000

# Gerar uma sugestão com base na classe prevista
if predicted_class == 0:
    suggestion = "Sugestão para a classe 0"
elif predicted_class == 1:
    suggestion = "Sugestão para a classe 1"
else:
    suggestion = "Sugestão para a classe 2"

# Truncar a sugestão se ela exceder o limite máximo de caracteres
if len(suggestion) > max_chars:
    suggestion = suggestion[:max_chars] + "..."

# Imprimir a sugestão gerada
print("Sugestão:", suggestion)
